{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import shapefile\n",
    "from dbfread import DBF\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPARE PLANET DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Planet directories\n",
    "# Metadata is *metadata.json\n",
    "\n",
    "ndvi_files = r'C:\\Users\\bpere\\Documents\\EMPIRE\\BARBARA\\Akademisch\\Remote_Sensing\\Thesis\\Py6S\\Fields\\official_dataset\\extra_birds\\extra_extra\\Planet\\ndvi'\n",
    "metadata_files = r'C:\\Users\\bpere\\Documents\\EMPIRE\\BARBARA\\Akademisch\\Remote_Sensing\\Thesis\\Py6S\\Fields\\official_dataset\\extra_birds\\extra_extra\\Planet\\metadata'\n",
    "csv_path = r'C:\\Users\\bpere\\Documents\\EMPIRE\\BARBARA\\Akademisch\\Remote_Sensing\\Thesis\\Py6S\\Fields\\official_dataset\\extra_birds\\extra_extra\\Planet\\csv\\fragments'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn .dbf into .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dbf_files(ndvi_path, metadata_path, output_path):\n",
    "    # Step 1: Locate metadata files\n",
    "    metadata_files = glob.glob(os.path.join(metadata_path, \"*_metadata.json\"))\n",
    "\n",
    "    for metadata_file in metadata_files:\n",
    "        # Extract base name from the metadata file\n",
    "        base_name = os.path.splitext(os.path.basename(metadata_file))[0]\n",
    "        matching_key = '_'.join(base_name.split('_')[:-1])  # Get the matching key up to the last underscore\n",
    "\n",
    "        # Step 2: Find the matching .dbf file in ndvi_path\n",
    "        matching_dbf_file = None\n",
    "\n",
    "        for dbf_file in os.listdir(ndvi_path):\n",
    "            if matching_key in dbf_file and dbf_file.endswith(\".dbf\"):\n",
    "                matching_dbf_file = os.path.join(ndvi_path, dbf_file)\n",
    "                break\n",
    "\n",
    "        if matching_dbf_file is not None:\n",
    "            # Convert the .dbf file to a pandas DataFrame\n",
    "            table = DBF(matching_dbf_file, encoding='utf-8')\n",
    "            df = pd.DataFrame(iter(table))\n",
    "\n",
    "            # Step 3: Add new columns to the DataFrame\n",
    "            df[\"date\"] = \"\"\n",
    "            df[\"cloud_cover\"] = \"\"\n",
    "\n",
    "            # Step 4: Extract \"cloud_percent\" and populate cloud_cover\n",
    "            with open(metadata_file, \"r\") as f:\n",
    "                metadata = json.load(f)\n",
    "                cloud_percent = metadata[\"properties\"][\"cloud_cover\"]\n",
    "                df[\"cloud_cover\"] = cloud_percent\n",
    "\n",
    "            # Step 5: Extract \"acquired\" and populate date\n",
    "            with open(metadata_file, \"r\") as f:\n",
    "                metadata = json.load(f)\n",
    "                acquired = metadata[\"properties\"][\"acquired\"]\n",
    "                df[\"date\"] = acquired.split(\"T\")[0]\n",
    "\n",
    "            # Step 6: Filter and rename columns\n",
    "            df = df[[\"date\", \"_median\", \"cloud_cover\", \"new_id\"]]\n",
    "            df = df.rename(columns={\"_median\": \"NDVI\"})\n",
    "            df = df.rename(columns={\"new_id\": \"field_id\"})\n",
    "\n",
    "            # Step 7: Filter out rows where NDVI is null or missing\n",
    "            df = df[df[\"NDVI\"].notnull()]\n",
    "\n",
    "            # Check if there are any rows left\n",
    "            if not df.empty:\n",
    "                # Step 8: Convert the DataFrame to CSV and save it\n",
    "                csv_file = os.path.join(output_path, base_name + \".csv\")\n",
    "                df.to_csv(csv_file, index=False)\n",
    "            else:\n",
    "                print(f\"No valid rows remaining for {base_name}, skipping CSV generation\")\n",
    "\n",
    "        else:\n",
    "            print(f\"No matching .dbf file found for {metadata_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dbf_files(ndvi_files, metadata_files, csv_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge Planet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_csv_files(csv_dir):\n",
    "    # Create an empty DataFrame to store the merged data\n",
    "    merged_df = pd.DataFrame()\n",
    "    \n",
    "    csv_files = glob.glob(os.path.join(csv_dir, \"*.csv\"))\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        merged_df = pd.concat([merged_df, df])\n",
    "    \n",
    "    merged_df = merged_df.reset_index(drop=True)\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date      NDVI  cloud_cover field_id\n",
      "0    2018-05-01  0.828534          0.0      RT1\n",
      "1    2018-05-01  0.840214          0.0      RT2\n",
      "2    2018-05-01  0.840210          0.0      RT3\n",
      "3    2018-05-01  0.854524          0.0      RT4\n",
      "4    2018-05-01  0.855745          0.0      RT5\n",
      "..          ...       ...          ...      ...\n",
      "655  2023-06-29  0.763382          0.0     DT53\n",
      "656  2023-06-29  0.444728          0.0     DT54\n",
      "657  2023-06-29  0.537545          0.0     DT55\n",
      "658  2023-06-29  0.876192          0.0     DT56\n",
      "659  2023-06-29  0.606636          0.0     DT58\n",
      "\n",
      "[660 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "planet_raw = merge_csv_files(csv_path)\n",
    "\n",
    "print(planet_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date      NDVI  cloud_cover field_id sat_name\n",
      "0    2018-05-01  0.828534          0.0      RT1   Planet\n",
      "1    2018-05-01  0.840104          0.0     RT10   Planet\n",
      "2    2018-05-01  0.708980          0.0     RT11   Planet\n",
      "3    2018-05-01  0.840214          0.0      RT2   Planet\n",
      "4    2018-05-01  0.840210          0.0      RT3   Planet\n",
      "..          ...       ...          ...      ...      ...\n",
      "655  2023-06-29  0.606636          0.0     DT58   Planet\n",
      "656  2023-06-29  0.766971          0.0      DT6   Planet\n",
      "657  2023-06-29  0.898831          0.0      DT7   Planet\n",
      "658  2023-06-29  0.601714          0.0      DT8   Planet\n",
      "659  2023-06-29  0.718538          0.0      DT9   Planet\n",
      "\n",
      "[660 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "unique_planet_raw = (\n",
    "    planet_raw\n",
    "    .dropna(subset = \"NDVI\")                                                    # drop rows w/ empty NDVI values\n",
    "    .groupby(['date', 'field_id'], as_index=False)[[\"NDVI\", \"cloud_cover\"]]\n",
    "    .median()                                                                   # calculate median for duplicate date-field_id combinations\n",
    "    .pipe(lambda x: x[[c for c in x if c != 'field_id'] + ['field_id']])\n",
    "    .assign(sat_name='Planet')\n",
    ")\n",
    "\n",
    "print(unique_planet_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_planet_raw.to_csv(r'C:\\Users\\bpere\\Documents\\EMPIRE\\BARBARA\\Akademisch\\Remote_Sensing\\Thesis\\Py6S\\Fields\\official_dataset\\extra_birds\\extra_extra\\Planet\\csv\\Pl_complete.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
